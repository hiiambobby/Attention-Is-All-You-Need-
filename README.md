# Attention Is All You Need: Transformer from Scratch

This repository contains a simple Transformer model implemented from scratch, based on the original paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al. The goal is to provide an educational and clear codebase that helps you understand the inner workings of the Transformer architecture without relying on heavy frameworks.

## Overview

Transformers have changed the way we process sequences, especially in NLP, by using self-attention mechanisms instead of recurrent or convolutional networks. This implementation focuses on the core concepts like multi-head self-attention, positional encoding, and feed-forward layers â€” all built from the ground up.

## Features

- Built from scratch with minimal dependencies
- Clear and modular code for easy understanding
- Covers all main components of the Transformer model
- Suitable for learning and experimentation


## Learning Resources

- [The Original Transformer Paper](https://arxiv.org/abs/1706.03762)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Hugging Face Course](https://huggingface.co/course)
